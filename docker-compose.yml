version: '3.9'

services:
  # ---------------------------------------------------------------------------
  # MIDDLEWARE LAYER (Abstraction)
  # ---------------------------------------------------------------------------
  middleware:
    build: 
      context: ./middleware
      dockerfile: Dockerfile
    container_name: miaa-middleware
    restart: always
    ports:
      - "8080:8080"
    environment:
      - INFERENCE_URL=http://inference:8000/v1
    # No depends_on, as inference is an alias

  # ---------------------------------------------------------------------------
  # MODEL 1: Ministral 3 14B (CUDA)
  # ---------------------------------------------------------------------------
  ministral-cuda:
    profiles: ["ministral", "default"]
    build:
      context: ./isolation
      dockerfile: Dockerfile.cuda
    container_name: miaa-ministral
    runtime: nvidia
    environment:
      - NVIDIA_VISIBLE_DEVICES=all
      - HF_TOKEN=${HF_TOKEN}
    volumes:
      - ~/.cache/huggingface:/root/.cache/huggingface
      - ./capsules:/app/capsules
    ports:
      - "8000:8000"
    # Base Model: Ministral 3 14B (Verified ID)
    command: --model cyankiwi/Ministral-3-14B-Instruct-2512-AWQ-4bit --port 8000 --dtype auto --gpu-memory-utilization 0.9 --quantization awq
    networks:
      default:
        aliases:
          - inference

  # ---------------------------------------------------------------------------
  # MODEL 2: GPT-OSS 20B (CUDA) - Thinking/CoT
  # ---------------------------------------------------------------------------
  gpt-cuda:
    profiles: ["gpt"]
    build:
      context: ./isolation
      dockerfile: Dockerfile.cuda
    container_name: miaa-gpt
    runtime: nvidia
    environment:
      - NVIDIA_VISIBLE_DEVICES=all
      - HF_TOKEN=${HF_TOKEN}
    volumes:
      - ~/.cache/huggingface:/root/.cache/huggingface
      - ./capsules:/app/capsules
    ports:
      - "8000:8000"
    # Base Model: GPT-OSS 20B
    command: --model openai/gpt-oss-20b --port 8000 --dtype auto --gpu-memory-utilization 0.9 --trust-remote-code --quantization bitsandbytes --load-format bitsandbytes
    networks:
      default:
        aliases:
          - inference

  # ---------------------------------------------------------------------------
  # MODEL 3: DeepSeek Distill 8B (CUDA)
  # ---------------------------------------------------------------------------
  deepseek-cuda:
    profiles: ["deepseek"]
    build:
      context: ./isolation
      dockerfile: Dockerfile.cuda
    container_name: miaa-deepseek
    runtime: nvidia
    environment:
      - NVIDIA_VISIBLE_DEVICES=all
      - HF_TOKEN=${HF_TOKEN}
    volumes:
      - ~/.cache/huggingface:/root/.cache/huggingface
      - ./capsules:/app/capsules
    ports:
      - "8000:8000"
    # Base Model: DeepSeek Distill (Qwen3-8B)
    command: --model TeichAI/Qwen3-8B-DeepSeek-v3.2-Speciale-Distill --port 8000 --dtype auto --gpu-memory-utilization 0.9 --trust-remote-code
    networks:
      default:
        aliases:
          - inference

networks:
  default:
    driver: bridge
